{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import joblib\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from datetime import datetime\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.applications import ResNet50, EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a variable for the dataset of sample (directory number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full dataset size\n",
    "\n",
    "# CT = 326 \n",
    "# CXR = 557\n",
    "\n",
    "# test dataset size \n",
    "\n",
    "CT =  20 \n",
    "CXR = 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = f\"{CT}_CT_{CXR}_CXR\"\n",
    "dataset_base_directory = f\"/home/masresha/dataset/all_dataset\"\n",
    "generated_data_directory = f\"/home/masresha/dataset/generatedfile/{CT}_CT_{CXR}_CXR_generated\"\n",
    "result_data_directory = f\"/home/masresha/dataset/resultfile/{CT}_CT_{CXR}_CXR_resultfile\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the base directory exists<br>\n",
    "Function to check if a directory exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_directory_exists(directory_path, create_if_missing=False):\n",
    "    if not os.path.exists(directory_path):\n",
    "        if create_if_missing:  # Create the directory if flagged\n",
    "            os.makedirs(directory_path)\n",
    "            print(f\"ðŸ“ Created directory: {directory_path}\")\n",
    "        else:  # Show error and exit if not allowed to create\n",
    "            print(f\"âŒ ERROR: Directory does not exist: {directory_path}. Exiting...\")\n",
    "            sys.exit(1)  # Abort execution\n",
    "    else:\n",
    "        print(f\"âœ… Directory exists: {directory_path}\")\n",
    "        return directory_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_directory_exists(dataset_base_directory, create_if_missing=False)  # Must exist\n",
    "ensure_directory_exists(generated_data_directory, create_if_missing=True)  # Create if missing\n",
    "ensure_directory_exists(result_data_directory, create_if_missing=True)  # Create if missing\n",
    "print(\"All directory checks and setups are complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define folder path and ensure it exists\n",
    "log_dir = os.path.join(result_data_directory, 'logs')\n",
    "ensure_directory_exists(log_dir, create_if_missing=True)\n",
    "\n",
    "# Configure logging\n",
    "log_filename = os.path.join(log_dir, f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")\n",
    "\n",
    "# Configure logging\n",
    "log_filename = f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    filename=log_filename,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "console = logging.StreamHandler()  # For console output\n",
    "console.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "console.setFormatter(formatter)\n",
    "logging.getLogger().addHandler(console)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load fused features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(d_type, feature_model, training_type):\n",
    "    if training_type == 'fused_data':   \n",
    "        load_dir = os.path.join(generated_data_directory,training_type, d_type,feature_model)\n",
    "        ensure_directory_exists(load_dir)\n",
    "        X = np.load(ensure_directory_exists(os.path.join(load_dir, f\"{d_type}_{feature_model}_fused_features.npy\")))\n",
    "        y = np.load(ensure_directory_exists(os.path.join(load_dir, f\"{d_type}_{feature_model}_labels.npy\")))\n",
    "        return X, y\n",
    "    elif training_type == 'clinical_data_only':\n",
    "        data_dir = os.path.join(generated_data_directory,'selected_features_csv',d_type)\n",
    "        ensure_directory_exists(data_dir)\n",
    "        data = pd.read_csv(ensure_directory_exists(os.path.join(data_dir, f\"{d_type}_clinical_data_selected_features_{feature_model}.csv\")))\n",
    "        X = data.drop(columns=['label'])\n",
    "        y = data['label']\n",
    "        return X, y\n",
    "    elif training_type == 'image_data_only':\n",
    "        data_dir = os.path.join(generated_data_directory,'selected_features_image_npy',d_type)\n",
    "        ensure_directory_exists(data_dir)\n",
    "        X = np.load(ensure_directory_exists(os.path.join(data_dir, f\"{d_type}_image_features.npy\")))\n",
    "        y = np.load(ensure_directory_exists(os.path.join(data_dir, f\"{d_type}_image_labels.npy\")))\n",
    "        return X, y\n",
    "    elif training_type == 'CNN':\n",
    "        data_dir = os.path.join(generated_data_directory, 'merged_preprocessed_image_npy',d_type)\n",
    "        ensure_directory_exists(data_dir)\n",
    "        X = np.load(ensure_directory_exists(os.path.join(data_dir, f\"{d_type}_image_data.npy\")))\n",
    "        y = np.load(ensure_directory_exists(os.path.join(data_dir, f\"{d_type}_image_labels.npy\")))\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(d_type, model_name, model, feature_model, training_type, save_type='plk'):\n",
    "    # Create 'saved_models' subfolder if it doesn't exist\n",
    "    model_dir = os.path.join(result_data_directory, training_type, 'saved_models',d_type)\n",
    "    ensure_directory_exists(model_dir, create_if_missing=True)\n",
    "\n",
    "    # Save model in 'saved_models' subfolder\n",
    "    if save_type == 'pkl':\n",
    "        joblib.dump(model, os.path.join(model_dir,f\"{d_type}_{training_type}_{feature_model}_{model_name}_model_sample_size_{d_type}.pkl\"))\n",
    "        print(f\"Model training and evaluation complete. Model saved as '{d_type}_{training_type}_{feature_model}_{model_name}_model_sample_size_{d_type}.pkl'\")\n",
    "    elif save_type == 'keras':\n",
    "        model.save(os.path.join(model_dir, f\"{d_type}_{training_type}_{feature_model}_{model_name}_model_sample_size_{d_type}.keras\"))   \n",
    "        print(f\"Model training and evaluation complete. Model saved as '{d_type}_{training_type}_{feature_model}_{model_name}_model_sample_size_{d_type}.keras'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def export_evaluation_result(d_type, y_true, y_pred, model_name, feature_model, inference_time_train, inference_time, evaluation_type, params, training_type):\n",
    "#     if evaluation_type != 'Eval_Test':\n",
    "#         acc = accuracy_score(y_true, y_pred)\n",
    "#         report = classification_report(y_true, y_pred, digits=6)\n",
    "#         cm = confusion_matrix(y_true, y_pred)\n",
    "#         print(f\"[{d_type}] [{feature_model}] [{training_type}] {model_name} {evaluation_type} Results (Sample size = {dataset}):\")\n",
    "#         print(f\"[{d_type}] [{feature_model}] [{training_type}] {evaluation_type} Accuracy:\", acc)\n",
    "#         print(f\"[{d_type}] [{feature_model}] [{training_type}] {evaluation_type} inference_time_train:\", inference_time_train)\n",
    "#         print(f\"[{d_type}] [{feature_model}] [{training_type}] {evaluation_type} inference_time_{evaluation_type}:\", inference_time)\n",
    "#         print(f\"{model_name} parameters: \\n {str(params)}\")\n",
    "#         print(f\"[{d_type}] [{feature_model}] {evaluation_type} Classification Report:\", report, '\\n')\n",
    "\n",
    "#         # Write results to a text file\n",
    "#         txt_dir = os.path.join(result_data_directory, training_type,'evaluation_result',d_type)\n",
    "#         ensure_directory_exists(txt_dir, create_if_missing=True)\n",
    "#         result_file = os.path.join(txt_dir, f\"{d_type}_{feature_model}_evaluation_results_{dataset}.txt\")\n",
    "#         with open(result_file, \"a\") as file:\n",
    "#             file.write(f\"\\n========================\\n[{d_type}]  [{training_type}] [{feature_model}] {model_name} {evaluation_type} Results (Sample size = {dataset}):\\n\")\n",
    "#             file.write(f\"[{model_name}{d_type}] {evaluation_type} parameters: {str(params)}\\n\")\n",
    "#             file.write(f\"[{d_type}] [{training_type}] {evaluation_type} Accuracy: {acc}\\n\")\n",
    "#             file.write(f\"[{d_type}] [{training_type}] {evaluation_type} inference_time_train: {inference_time_train}\\n\")\n",
    "#             file.write(f\"[{d_type}] [{training_type}] {evaluation_type} inference_time_{evaluation_type}: {inference_time}\\n\")\n",
    "#             file.write(f\"[{d_type}] [{training_type}] {evaluation_type} Classification Report: \\n{report}\\n\")\n",
    "#             file.write(f\"[{d_type}] [{training_type}] {evaluation_type} Confusion Matrix:\\n\")\n",
    "#             cm_df = pd.DataFrame(cm)  # Create a pandas DataFrame\n",
    "#             file.write(cm_df.to_string())  # Convert the DataFrame to a string\n",
    "#             file.write(\"\\n\")\n",
    "#         print(f\"[{d_type}]  [{training_type}] [{feature_model}] {model_name} {evaluation_type} results saved to {result_file}\")\n",
    "\n",
    "#         # Plot confusion matrix\n",
    "#         plt.figure()\n",
    "#         sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "#         plt.xlabel('Predicted')\n",
    "#         plt.ylabel('Actual')\n",
    "#         plt.title(f'[{d_type}] [{feature_model}] [{training_type}] {model_name} {evaluation_type} \\nConfusion Matrix (Sample size = {dataset})')\n",
    "\n",
    "#         # Save confusion matrix\n",
    "#         plt.tight_layout()\n",
    "#         confusion_matrix_dir = os.path.join(result_data_directory, training_type,'evaluation_result',d_type,f'{d_type}_{feature_model}_confusion_matrix')\n",
    "#         ensure_directory_exists(confusion_matrix_dir, True)\n",
    "#         confusion_matrix_file = os.path.join(confusion_matrix_dir, f\"{d_type}_{feature_model}_{model_name}_{evaluation_type}_confusion_matrix_sample_size_{dataset}.png\")\n",
    "#         plt.savefig(confusion_matrix_file)\n",
    "#         plt.close()\n",
    "#         print(f\"[{d_type}] [{feature_model}] {model_name} {training_type} {evaluation_type} \\nConfusion matrix saved to {confusion_matrix_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train_model(learning_type,X_train, y_train, X_val, y_val, model,model_name, feature_model, training_type, batch_size=3, epochs=5):\n",
    "    if learning_type =='ML': \n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        inference_time_train = time.time() - start_time\n",
    "        return model, inference_time_train\n",
    "    elif learning_type == 'DL':\n",
    "        # model = build_deep_learning_model(input_dim=X_train.shape[1], output_dim=len(np.unique(y_train)))\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        start_time = time.time()\n",
    "        history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size,\n",
    "                            callbacks=[early_stopping], verbose=1)\n",
    "        inference_time_train = time.time() - start_time\n",
    "        return model, inference_time_train, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(learning_type, model, X_test, y_test):\n",
    "    if learning_type == 'ML':\n",
    "        start_time = time.time()\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        end_time = time.time()\n",
    "        inference_time_test = end_time - start_time\n",
    "        return inference_time_test\n",
    "    elif learning_type ==\"DL\":\n",
    "        start_time = time.time()\n",
    "        dl_eval = model.evaluate(X_test, y_test, verbose=0)\n",
    "        inference_time_test = time.time() - start_time\n",
    "        return dl_eval, inference_time_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluation_summary(d_type, feature_model, training_type, model_name, evaluation_type, acc, inference_time_train, inference_time, report, params):\n",
    "    \"\"\"\n",
    "    Print evaluation summary to the console.\n",
    "    \"\"\"\n",
    "    print(f\"[{d_type}] [{feature_model}] [{training_type}] {model_name} {evaluation_type} Results (Sample size = {dataset}):\")\n",
    "    print(f\"[{d_type}] [{feature_model}] [{training_type}] {evaluation_type} Accuracy:\", acc)\n",
    "    print(f\"[{d_type}] [{feature_model}] [{training_type}] {evaluation_type} inference_time_train:\", inference_time_train)\n",
    "    print(f\"[{d_type}] [{feature_model}] [{training_type}] {evaluation_type} inference_time_{evaluation_type}:\", inference_time)\n",
    "    print(f\"{model_name} parameters: \\n {str(params)}\")\n",
    "    print(f\"[{d_type}] [{feature_model}] {evaluation_type} Classification Report:\", report, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_file(d_type, feature_model, training_type, model_name, evaluation_type, acc, inference_time_train, inference_time, report, cm, params):\n",
    "    \"\"\"\n",
    "    Save the evaluation results to a text file.\n",
    "    \"\"\"\n",
    "    txt_dir = os.path.join(result_data_directory, training_type, 'evaluation_result', d_type)\n",
    "    ensure_directory_exists(txt_dir, create_if_missing=True)\n",
    "    result_file = os.path.join(txt_dir, f\"{d_type}_{feature_model}_evaluation_results_{dataset}.txt\")\n",
    "    \n",
    "    with open(result_file, \"a\") as file:\n",
    "        file.write(f\"\\n========================\\n[{d_type}]  [{training_type}] [{feature_model}] {model_name} {evaluation_type} Results (Sample size = {dataset}):\\n\")\n",
    "        file.write(f\"[{model_name}{d_type}] {evaluation_type} parameters: {str(params)}\\n\")\n",
    "        file.write(f\"[{d_type}] [{training_type}] {evaluation_type} Accuracy: {acc}\\n\")\n",
    "        file.write(f\"[{d_type}] [{training_type}] {evaluation_type} inference_time_train: {inference_time_train}\\n\")\n",
    "        file.write(f\"[{d_type}] [{training_type}] {evaluation_type} inference_time_{evaluation_type}: {inference_time}\\n\")\n",
    "        file.write(f\"[{d_type}] [{training_type}] {evaluation_type} Classification Report: \\n{report}\\n\")\n",
    "        file.write(f\"[{d_type}] [{training_type}] {evaluation_type} Confusion Matrix:\\n\")\n",
    "        cm_df = pd.DataFrame(cm)  # Create a pandas DataFrame\n",
    "        file.write(cm_df.to_string())  # Convert the DataFrame to a string\n",
    "        file.write(\"\\n\")\n",
    "    \n",
    "    print(f\"[{d_type}]  [{training_type}] [{feature_model}] {model_name} {evaluation_type} results saved to {result_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save_confusion_matrix(d_type, feature_model, model_name, evaluation_type, cm, dataset, training_type):\n",
    "    \"\"\"\n",
    "    Plot and save the confusion matrix.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'[{d_type}] [{feature_model}] [{training_type}] {model_name} {evaluation_type} \\nConfusion Matrix (Sample size = {dataset})')\n",
    "\n",
    "    # Save confusion matrix\n",
    "    plt.tight_layout()\n",
    "    confusion_matrix_dir = os.path.join(result_data_directory, training_type, 'evaluation_result', d_type, f'{d_type}_{feature_model}_confusion_matrix')\n",
    "    ensure_directory_exists(confusion_matrix_dir, True)\n",
    "    confusion_matrix_file = os.path.join(confusion_matrix_dir, f\"{d_type}_{feature_model}_{model_name}_{evaluation_type}_confusion_matrix_sample_size_{dataset}.png\")\n",
    "    plt.savefig(confusion_matrix_file)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"[{d_type}] [{feature_model}] {model_name} {training_type} {evaluation_type} \\nConfusion matrix saved to {confusion_matrix_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(history, d_type, feature_model, model_name, training_type, dataset, graph_type):\n",
    "    \"\"\"\n",
    "    Plots and saves training/validation accuracy and loss graphs.\n",
    "    \n",
    "    Parameters:\n",
    "    - history: Training history containing 'accuracy', 'val_accuracy', 'loss', and 'val_loss'\n",
    "    - d_type: Dataset type for the plot title and file name\n",
    "    - feature_model: Feature extraction model used in training\n",
    "    - model_name: Name of the model being evaluated\n",
    "    - training_type: Type of training used (e.g., 'fine-tuning', 'transfer')\n",
    "    - dataset: Dataset used for training\n",
    "    - graph_type: Type of graph to plot ('accuracy' or 'loss')\n",
    "    \"\"\"\n",
    "    if graph_type == 'accuracy':\n",
    "        metric = 'accuracy'\n",
    "        val_metric = 'val_accuracy'\n",
    "        title = f'[{d_type}]Training and validation accuracy of model: {model_name}, \\nFeature extraction: {feature_model}, dataset type: {training_type}'\n",
    "    elif graph_type == 'loss':\n",
    "        metric = 'loss'\n",
    "        val_metric = 'val_loss'\n",
    "        title = f'[{d_type}]Training and validation loss of model: {model_name}, \\nFeature extraction: {feature_model}, dataset type: {training_type}'\n",
    "    else:\n",
    "        raise ValueError(\"graph_type should be either 'accuracy' or 'loss'\")\n",
    "    \n",
    "    # Extract data from history\n",
    "    metric_data = history.history[metric]\n",
    "    val_metric_data = history.history[val_metric]\n",
    "    \n",
    "    epochs = range(1, len(metric_data) + 1)\n",
    "\n",
    "    # Plot the graph\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, metric_data, label=f'Training {graph_type}')\n",
    "    plt.plot(epochs, val_metric_data, label=f'Validation {graph_type}')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "    # Save the plot\n",
    "    plt.tight_layout()\n",
    "    training_plots_dir = os.path.join(result_data_directory, training_type, 'evaluation_result', d_type, f'{d_type}_{feature_model}_training_plots')\n",
    "    ensure_directory_exists(training_plots_dir, True)\n",
    "    plot_file = os.path.join(training_plots_dir, f\"{d_type}_{feature_model}_{model_name}_{graph_type}_training_plots_sample_size_{dataset}.png\")\n",
    "    plt.savefig(plot_file)\n",
    "    print(f\"[{d_type}] [{feature_model or ''}] {model_name} {training_type} '{graph_type}' training plots saved to {plot_file}\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_deep_learning_model(input_dim, output_dim):\n",
    "    # a fully connected (dense) neural network\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=input_dim),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(output_dim, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_resnet50_model(input_shape, output_dim):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False  # Freeze the base model\n",
    "\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(output_dim, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_efficientnetb0_model(input_shape, output_dim):\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False  # Freeze the base model\n",
    "\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(output_dim, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_evaluation_result(d_type, y_true, y_pred, model_name, feature_model, inference_time_train, inference_time, evaluation_type, params, training_type):\n",
    "    \"\"\"\n",
    "    Export evaluation results including accuracy, classification report, confusion matrix, and more.\n",
    "    \"\"\"\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, digits=6)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    print_evaluation_summary(d_type, feature_model, training_type, model_name, evaluation_type, acc, inference_time_train, inference_time, report, params)\n",
    "\n",
    "    save_results_to_file(d_type, feature_model, training_type, model_name, evaluation_type, acc, inference_time_train, inference_time, report, cm, params)\n",
    "\n",
    "    plot_and_save_confusion_matrix(d_type, feature_model, model_name, evaluation_type, cm, dataset, training_type)\n",
    "    \n",
    "    # plot_graph(history, d_type, feature_model, model_name, training_type, dataset, graph_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_evaluate(learning_type,model, X_val, y_val, evaluation_type, model_name, feature_model, training_type, d_type, dataset, inference_time_train):\n",
    "    if learning_type == 'ML':\n",
    "        start_time = time.time()\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        end_time = time.time()\n",
    "        inference_time = end_time - start_time\n",
    "        export_evaluation_result(d_type, y_val, y_val_pred, model_name, feature_model, inference_time_train, inference_time, evaluation_type, model_name, training_type)\n",
    "\n",
    "    elif learning_type == 'DL':\n",
    "        start_time = time.time()\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        y_val_pred = np.argmax(y_val_pred, axis=1)\n",
    "        inference_time = time.time() - start_time\n",
    "        export_evaluation_result(d_type, y_val, y_val_pred, model_name, feature_model, inference_time_train, inference_time, evaluation_type, model_name, training_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop with validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of loops for progress tracking\n",
    "total_dtypes = len(['CT', 'CXR'])\n",
    "total_feature_models = len(['mi', 'rf', 'pca'])\n",
    "total_training_types = len(['fused_data', 'clinical_data_only', 'image_data_only'])\n",
    "total_classifiers = len([\n",
    "    'GradientBoosting', 'RandomForest', 'XGBoost', 'LightGBM', 'CatBoost', \n",
    "    'MLP', 'SVM', 'LogisticRegression', 'KNN'\n",
    "])\n",
    "total_cnn_models = len(['ResNet50', 'EfficientNetB0'])\n",
    "\n",
    "\n",
    "\n",
    "for dtype_idx, d_type in enumerate(['CT', 'CXR'], start=1):\n",
    "    logging.info(f\"Processing data type ({dtype_idx}/{total_dtypes}): {d_type}\")\n",
    "\n",
    "    feature_extraction_models = ['mi', 'rf', 'pca']\n",
    "    for feature_idx, feature_model in enumerate(['mi', 'rf', 'pca'], start=1):\n",
    "        logging.info(f\"    Feature model ({feature_idx}/{total_feature_models}): {feature_model}\")\n",
    "        # Define parameters\n",
    "        parameters = {\n",
    "            'GradientBoosting': {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 3, 'random_state': 42},\n",
    "            'RandomForest': {'n_estimators': 100, 'random_state': 42},\n",
    "            'XGBoost': {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 3, 'random_state': 42},\n",
    "            'LightGBM': {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 3, 'random_state': 42},\n",
    "            'CatBoost': {'iterations': 100, 'learning_rate': 0.1, 'depth': 3, 'random_state': 42, 'verbose': 0},\n",
    "            'MLP': {'hidden_layer_sizes': (100,), 'max_iter': 500, 'random_state': 42},\n",
    "            'SVM': {'kernel': 'rbf', 'C': 1, 'gamma': 'scale', 'probability': True},\n",
    "            'LogisticRegression': {'max_iter': 500, 'random_state': 42},  # Shorter key for consistency\n",
    "            'KNN': {'n_neighbors': 5}\n",
    "        }\n",
    "\n",
    "        # Define classifiers\n",
    "        classifiers = {}\n",
    "        classifiers['GradientBoosting'] = GradientBoostingClassifier(**parameters['GradientBoosting'])\n",
    "        classifiers['RandomForest'] = RandomForestClassifier(**parameters['RandomForest'])\n",
    "        classifiers['XGBoost'] = XGBClassifier(**parameters['XGBoost'])\n",
    "        classifiers['LightGBM'] = LGBMClassifier(**parameters['LightGBM'])\n",
    "        classifiers['CatBoost'] = CatBoostClassifier(**parameters['CatBoost'])\n",
    "        classifiers['MLP'] = MLPClassifier(**parameters['MLP'])\n",
    "        classifiers['SVM'] = SVC(**parameters['SVM'])\n",
    "        classifiers['LogisticRegression'] = LogisticRegression(**parameters['LogisticRegression'])\n",
    "        classifiers['KNN'] = KNeighborsClassifier(**parameters['KNN'])\n",
    "\n",
    "        for train_idx, training_type in enumerate(['fused_data', 'clinical_data_only', 'image_data_only'], start=1):\n",
    "            loops_remaining = (\n",
    "                total_dtypes * total_feature_models * total_training_types\n",
    "                - (dtype_idx - 1) * total_feature_models * total_training_types\n",
    "                - (feature_idx - 1) * total_training_types\n",
    "                - train_idx\n",
    "            )\n",
    "            logging.info(f\"        Training type ({train_idx}/{total_training_types}): {training_type} \"\n",
    "                         f\"(Loops remaining: {loops_remaining})\")\n",
    "\n",
    "   \n",
    "            # Load data\n",
    "            X, y = load_data(d_type, feature_model, training_type)\n",
    "            \n",
    "            # Step 1: Split into training (80%) and temporary set (20%)\n",
    "            X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "                X, y, test_size=0.2,random_state=42, stratify=y\n",
    "            )\n",
    "\n",
    "            # Step 2: Split the temporary set into testing (10%) and validation (10%)\n",
    "            X_test, X_val, y_test, y_val = train_test_split(\n",
    "                X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    "            )\n",
    "            \n",
    "            # --> fused_data, clinical_data_only , image_data_only\n",
    "            ## Train and evaluate classifiers Machine learning models\n",
    "            for model_idx, (model_name, model) in enumerate(classifiers.items(), start=1):\n",
    "                logging.info(f\"            Training ML model ({model_idx}/{total_classifiers}): {model_name}\")\n",
    "                logging.info(f\"                Remaining ML models: {total_classifiers - model_idx}\")\n",
    "\n",
    "                \n",
    "                model_ml, inference_time_train = build_and_train_model('ML',X_train, y_train, X_val, y_val, model,model_name, feature_model, training_type)\n",
    "                inference_time_test = evaluate_model('ML',model_ml, X_test, y_test)\n",
    "                predict_and_evaluate('ML',model_ml, X_val, y_val, \"Validation\", model_name, feature_model, training_type, d_type, dataset, inference_time_train)\n",
    "                save_model(d_type, model_name, model, feature_model, training_type)\n",
    "                print(f\"{training_type} training completed and result is saved\")\n",
    "\n",
    "            ## deep learning  using Fully connected modail working on feature extracted data\n",
    "            if training_type == 'clinical_data_only':\n",
    "                logging.info(f\"        Skipping deep learning for training type: {training_type}\")\n",
    "                continue  ## --> fused_data and image_data_only\n",
    "            \n",
    "            logging.info(f\"        Training deep learning model: Fully_Connected\")\n",
    "            model_name = 'Fully_Connected'\n",
    "            model_dl = build_deep_learning_model(input_dim=X_train.shape[1], output_dim=len(np.unique(y)))\n",
    "\n",
    "            model_dl, inference_time_train, history = build_and_train_model('DL',X_train, y_train, X_val, y_val, model_dl,model_name, feature_model, training_type)\n",
    "            dl_eval, inference_time_test = evaluate_model('DL',model_dl, X_test, y_test)\n",
    "            predict_and_evaluate('DL',model_dl, X_val, y_val, \"Validation\", model_name, feature_model, training_type, d_type, dataset, inference_time_train)\n",
    "            save_model(d_type, model_name, model_dl, feature_model, training_type, save_type='keras')\n",
    "            \n",
    "            acc = history.history['accuracy']\n",
    "            val_acc = history.history['val_accuracy']\n",
    "            loss = history.history['loss']\n",
    "            val_loss = history.history['val_loss']\n",
    "\n",
    "            epochs = range(1, len(acc) + 1)\n",
    "            # plot figures models\n",
    "            plot_graph(history, d_type, feature_model, model_name, training_type, dataset, 'accuracy')\n",
    "            plot_graph(history, d_type, feature_model, model_name, training_type, dataset, 'loss')\n",
    "            \n",
    "    ## --> image only  , withou fusing the data\n",
    "    for training_type in ['image_data_only']:\n",
    "        # Train CNN Models for Image Data\n",
    "        # Load data\n",
    "        X, y = load_data(d_type, None, 'CNN')\n",
    "        X = np.expand_dims(X, axis=-1) # Add a single channel for grayscale\n",
    "        X = np.repeat(X, 3, axis=-1)  # Duplicate the single channel to simulate RGB\n",
    "        # Print the mapping\n",
    "        encoder = LabelEncoder()\n",
    "        y = encoder.fit_transform(y)  # Encodes labels as integers\n",
    "        label_mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "        print(f\"Label Mapping: {label_mapping}\")\n",
    "        \n",
    "        # Step 1: Split into training (80%) and temporary set (20%)\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "            X, y, test_size=0.2,random_state=42, stratify=y\n",
    "        )\n",
    "\n",
    "        # Step 2: Split the temporary set into testing (10%) and validation (10%)\n",
    "        X_test, X_val, y_test, y_val = train_test_split(\n",
    "            X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    "        )\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        height, width, channels = 224, 224, 3\n",
    "        \n",
    "        cnn_models = {\"ResNet50\": build_resnet50_model, \"EfficientNetB0\": build_efficientnetb0_model}\n",
    "        total_cnn_models = len(cnn_models)\n",
    "        for cnn_idx, (cnn_model_name, cnn_model_builder) in enumerate(cnn_models.items(), start=1):\n",
    "            logging.info(f\"            Training CNN model ({cnn_idx}/{total_cnn_models}): {cnn_model_name}\")\n",
    "            logging.info(f\"                Remaining CNN models: {total_cnn_models - cnn_idx}\")\n",
    "            \n",
    "            cnn_model = cnn_model_builder(input_shape=(height, width, channels), output_dim=len(np.unique(y_train)))\n",
    "            model_dl, inference_time_train, history = build_and_train_model('DL',X_train, y_train, X_val, y_val, cnn_model,cnn_model_name, None, training_type)\n",
    "\n",
    "            dl_eval, inference_time_test = evaluate_model('DL',model_dl, X_test, y_test)\n",
    "            predict_and_evaluate('DL',model_dl, X_val, y_val, \"Validation\", cnn_model_name, None, training_type, d_type, dataset, inference_time_train)\n",
    "            save_model(d_type, cnn_model_name, model_dl, None, training_type, save_type='keras')\n",
    "                        \n",
    "            acc = history.history['accuracy']\n",
    "            val_acc = history.history['val_accuracy']\n",
    "            loss = history.history['loss']\n",
    "            val_loss = history.history['val_loss']\n",
    "\n",
    "            epochs = range(1, len(acc) + 1)\n",
    "            # plot figures models\n",
    "            plot_graph(history, d_type, None, cnn_model_name, training_type, dataset, 'accuracy')\n",
    "            plot_graph(history, d_type, None, cnn_model_name, training_type, dataset, 'loss')\n",
    "            \n",
    "\n",
    "    logging.info(f\"Completed processing for data type: {d_type}\")\n",
    "\n",
    "            \n",
    "logging.info(\"[Done] All models evaluated and results saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
